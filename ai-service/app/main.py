from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import os
from datetime import datetime
import openai
import logging
from dotenv import load_dotenv
import json
import asyncio

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é…ç½®OpenAI
openai.api_key = os.getenv('OPENAI_API_KEY')
if not openai.api_key:
    logger.warning("âš ï¸ OPENAI_API_KEY æœªè®¾ç½®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

app = FastAPI(
    title="é‚®ç®±ç®¡å®¶ AI æœåŠ¡",
    description="æ™ºèƒ½é‚®ä»¶åˆ†æå’ŒAIåŠ©æ‰‹æœåŠ¡",
    version="1.0.0"
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•°æ®æ¨¡å‹
class EmailAnalysisRequest(BaseModel):
    email_id: str
    subject: str
    content: str
    sender: str

class EmailAnalysisResponse(BaseModel):
    email_id: str
    summary: str
    priority: str  # high, medium, low
    sentiment: str  # positive, neutral, negative
    suggested_reply: Optional[str] = None
    tags: List[str] = []
    confidence: float = 0.0
    key_points: List[str] = []
    action_required: bool = False

class ChatMessage(BaseModel):
    message: str
    context: Optional[str] = None

class ChatResponse(BaseModel):
    reply: str
    timestamp: str
    suggestions: List[str] = []

class EmailBatch(BaseModel):
    emails: List[EmailAnalysisRequest]

class BatchAnalysisResponse(BaseModel):
    results: List[EmailAnalysisResponse]
    summary_stats: dict

class HealthResponse(BaseModel):
    status: str
    timestamp: str
    version: str

@app.get("/", response_model=dict)
async def root():
    """æ ¹è·¯å¾„ï¼Œè¿”å›æœåŠ¡ä¿¡æ¯"""
    return {
        "message": "ğŸ“§ é‚®ç®±ç®¡å®¶ AI æœåŠ¡",
        "version": "1.0.0",
        "status": "running",
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return HealthResponse(
        status="ok",
        timestamp=datetime.now().isoformat(),
        version="1.0.0"
    )

# AIåŠ©æ‰‹åŠŸèƒ½
async def get_openai_analysis(subject: str, content: str, sender: str) -> dict:
    """ä½¿ç”¨OpenAIåˆ†æé‚®ä»¶å†…å®¹"""
    if not openai.api_key:
        # æ¨¡æ‹Ÿå“åº”
        return {
            "summary": f"è¿™æ˜¯ä¸€å°å…³äº '{subject}' çš„é‚®ä»¶ï¼Œæ¥è‡ª {sender}",
            "priority": "medium",
            "sentiment": "neutral",
            "suggested_reply": "è°¢è°¢æ‚¨çš„é‚®ä»¶ï¼Œæˆ‘ä¼šä»”ç»†é˜…è¯»å¹¶å›å¤ã€‚",
            "tags": ["å·¥ä½œ", "å¾…å›å¤"],
            "confidence": 0.7,
            "key_points": ["éœ€è¦å›å¤", "æŸ¥çœ‹è¯¦æƒ…"],
            "action_required": True
        }
    
    try:
        # æ„å»ºæç¤ºè¯
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹é‚®ä»¶å†…å®¹å¹¶æä¾›ç»“æ„åŒ–åˆ†æï¼š
        
        å‘ä»¶äººï¼š{sender}
        ä¸»é¢˜ï¼š{subject}
        å†…å®¹ï¼š{content}
        
        è¯·æä¾›ä»¥ä¸‹åˆ†æï¼ˆç”¨JSONæ ¼å¼è¿”å›ï¼‰ï¼š
        1. summary: é‚®ä»¶å†…å®¹æ‘˜è¦ï¼ˆ50å­—ä»¥å†…ï¼‰
        2. priority: ä¼˜å…ˆçº§ï¼ˆhigh/medium/lowï¼‰
        3. sentiment: æƒ…æ„Ÿåˆ†æï¼ˆpositive/neutral/negativeï¼‰
        4. suggested_reply: å»ºè®®å›å¤å†…å®¹ï¼ˆå¯é€‰ï¼‰
        5. tags: ç›¸å…³æ ‡ç­¾ï¼ˆæœ€å¤š5ä¸ªï¼‰
        6. confidence: åˆ†æç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
        7. key_points: å…³é”®è¦ç‚¹ï¼ˆæœ€å¤š3ä¸ªï¼‰
        8. action_required: æ˜¯å¦éœ€è¦è¡ŒåŠ¨ï¼ˆtrue/falseï¼‰
        """
        
        response = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‚®ä»¶åˆ†æåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå‡†ç¡®åˆ†æé‚®ä»¶å†…å®¹å¹¶æä¾›æœ‰ç”¨çš„å»ºè®®ã€‚è¯·ç”¨JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
        )
        
        result = response.choices[0].message.content
        # å°è¯•è§£æJSONå“åº”
        try:
            return json.loads(result)
        except json.JSONDecodeError:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æ„
            return {
                "summary": f"é‚®ä»¶ä¸»é¢˜ï¼š{subject}",
                "priority": "medium",
                "sentiment": "neutral",
                "suggested_reply": None,
                "tags": ["AIåˆ†æ"],
                "confidence": 0.5,
                "key_points": ["éœ€è¦äººå·¥å®¡æŸ¥"],
                "action_required": False
            }
    except Exception as e:
        logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}")
        return {
            "summary": f"é‚®ä»¶æ¥è‡ª{sender}ï¼Œä¸»é¢˜ï¼š{subject}",
            "priority": "medium",
            "sentiment": "neutral",
            "suggested_reply": None,
            "tags": ["åˆ†æå¤±è´¥"],
            "confidence": 0.0,
            "key_points": ["AIåˆ†æä¸å¯ç”¨"],
            "action_required": False
        }

async def get_ai_chat_response(message: str, context: str = None) -> dict:
    """AIèŠå¤©åŠŸèƒ½"""
    if not openai.api_key:
        # æ¨¡æ‹ŸAIå›å¤
        if "é‚®ä»¶" in message:
            return {
                "reply": "æˆ‘å¯ä»¥å¸®æ‚¨åˆ†æå’Œç®¡ç†é‚®ä»¶ã€‚æ‚¨æƒ³äº†è§£ä»€ä¹ˆå…·ä½“ä¿¡æ¯ï¼Ÿ",
                "suggestions": ["æŸ¥çœ‹æœªè¯»é‚®ä»¶", "é‚®ä»¶ç»Ÿè®¡", "ä¼˜å…ˆçº§é‚®ä»¶"]
            }
        elif "ç»Ÿè®¡" in message:
            return {
                "reply": "æ ¹æ®æœ€æ–°æ•°æ®ï¼Œæ‚¨æœ‰25å°æœªè¯»é‚®ä»¶ï¼Œå…¶ä¸­5å°æ˜¯é«˜ä¼˜å…ˆçº§çš„ã€‚",
                "suggestions": ["æŸ¥çœ‹é«˜ä¼˜å…ˆçº§", "æ‰¹é‡å¤„ç†", "è®¾ç½®æé†’"]
            }
        else:
            return {
                "reply": "æˆ‘æ˜¯æ‚¨çš„AIé‚®ä»¶åŠ©æ‰‹ï¼Œå¯ä»¥å¸®æ‚¨ç®¡ç†é‚®ä»¶ã€åˆ†æå†…å®¹å’Œæä¾›å»ºè®®ã€‚",
                "suggestions": ["é‚®ä»¶åˆ†æ", "æ™ºèƒ½å›å¤", "é‚®ä»¶ç»Ÿè®¡"]
            }
    
    try:
        system_message = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‚®ä»¶ç®¡ç†åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·åˆ†æé‚®ä»¶ã€æä¾›å»ºè®®å’Œå›ç­”ç›¸å…³é—®é¢˜ã€‚è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œè¯­æ°”å‹å¥½ä¸“ä¸šã€‚"
        if context:
            system_message += f" ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}"
        
        response = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": message}
                ],
                max_tokens=300,
                temperature=0.7
            )
        )
        
        reply = response.choices[0].message.content
        
        # ç”Ÿæˆç›¸å…³å»ºè®®
        suggestions = []
        if "é‚®ä»¶" in message.lower():
            suggestions = ["æŸ¥çœ‹é‚®ä»¶åˆ—è¡¨", "åˆ†æé‚®ä»¶å†…å®¹", "è®¾ç½®é‚®ä»¶è§„åˆ™"]
        elif "ç»Ÿè®¡" in message.lower():
            suggestions = ["è¯¦ç»†ç»Ÿè®¡", "è¶‹åŠ¿åˆ†æ", "å¯¼å‡ºæŠ¥å‘Š"]
        else:
            suggestions = ["é‚®ä»¶ç®¡ç†", "AIåˆ†æ", "å¸®åŠ©æ–‡æ¡£"]
        
        return {
            "reply": reply,
            "suggestions": suggestions
        }
    except Exception as e:
        logger.error(f"AIèŠå¤©å¤±è´¥: {str(e)}")
        return {
            "reply": "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚",
            "suggestions": ["é‡è¯•", "æŸ¥çœ‹å¸®åŠ©", "è”ç³»æ”¯æŒ"]
        }
@app.post("/analyze/email", response_model=EmailAnalysisResponse)
async def analyze_email(request: EmailAnalysisRequest):
    """åˆ†æé‚®ä»¶å†…å®¹"""
    try:
        logger.info(f"å¼€å§‹åˆ†æé‚®ä»¶: {request.email_id}")
        
        # ä½¿ç”¨AIåˆ†æé‚®ä»¶
        ai_result = await get_openai_analysis(
            request.subject, 
            request.content, 
            request.sender
        )
        
        # æ„å»ºå“åº”
        analysis = EmailAnalysisResponse(
            email_id=request.email_id,
            summary=ai_result.get("summary", "æœªçŸ¥å†…å®¹"),
            priority=ai_result.get("priority", "medium"),
            sentiment=ai_result.get("sentiment", "neutral"),
            suggested_reply=ai_result.get("suggested_reply"),
            tags=ai_result.get("tags", []),
            confidence=ai_result.get("confidence", 0.0),
            key_points=ai_result.get("key_points", []),
            action_required=ai_result.get("action_required", False)
        )
        
        logger.info(f"é‚®ä»¶åˆ†æå®Œæˆ: {request.email_id}, ä¼˜å…ˆçº§: {analysis.priority}")
        return analysis
        
    except Exception as e:
        logger.error(f"åˆ†æé‚®ä»¶æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ†æé‚®ä»¶æ—¶å‡ºé”™: {str(e)}")

@app.post("/analyze/batch", response_model=BatchAnalysisResponse)
async def analyze_batch_emails(batch: EmailBatch):
    """æ‰¹é‡åˆ†æé‚®ä»¶"""
    try:
        logger.info(f"å¼€å§‹æ‰¹é‡åˆ†æ {len(batch.emails)} å°é‚®ä»¶")
        
        results = []
        priorities = {"high": 0, "medium": 0, "low": 0}
        sentiments = {"positive": 0, "neutral": 0, "negative": 0}
        
        # å¹¶å‘å¤„ç†å¤šå°é‚®ä»¶
        tasks = []
        for email_req in batch.emails:
            task = get_openai_analysis(email_req.subject, email_req.content, email_req.sender)
            tasks.append((email_req.email_id, task))
        
        # ç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆ
        for email_id, task in tasks:
            ai_result = await task
            
            analysis = EmailAnalysisResponse(
                email_id=email_id,
                summary=ai_result.get("summary", "æœªçŸ¥å†…å®¹"),
                priority=ai_result.get("priority", "medium"),
                sentiment=ai_result.get("sentiment", "neutral"),
                suggested_reply=ai_result.get("suggested_reply"),
                tags=ai_result.get("tags", []),
                confidence=ai_result.get("confidence", 0.0),
                key_points=ai_result.get("key_points", []),
                action_required=ai_result.get("action_required", False)
            )
            
            results.append(analysis)
            
            # ç»Ÿè®¡ä¿¡æ¯
            priorities[analysis.priority] += 1
            sentiments[analysis.sentiment] += 1
        
        summary_stats = {
            "total_emails": len(results),
            "priority_distribution": priorities,
            "sentiment_distribution": sentiments,
            "action_required_count": sum(1 for r in results if r.action_required),
            "avg_confidence": sum(r.confidence for r in results) / len(results) if results else 0
        }
        
        logger.info(f"æ‰¹é‡åˆ†æå®Œæˆ: {len(results)} å°é‚®ä»¶")
        return BatchAnalysisResponse(results=results, summary_stats=summary_stats)
        
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")
@app.get("/stats/summary")
async def get_email_stats():
    """è·å–é‚®ä»¶ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "total_emails": 150,
        "unread_emails": 25,
        "high_priority": 5,
        "medium_priority": 15,
        "low_priority": 5,
        "sentiment_distribution": {
            "positive": 40,
            "neutral": 50,
            "negative": 10
        },
        "ai_analysis_status": "active" if openai.api_key else "mock_mode"
    }

@app.post("/ai/chat", response_model=ChatResponse)
async def ai_chat(message: ChatMessage):
    """AIèŠå¤©æ¥å£"""
    try:
        logger.info(f"AIèŠå¤©è¯·æ±‚: {message.message[:50]}...")
        
        # ä½¿ç”¨AIç”Ÿæˆå›å¤
        chat_result = await get_ai_chat_response(message.message, message.context)
        
        response = ChatResponse(
            reply=chat_result["reply"],
            timestamp=datetime.now().isoformat(),
            suggestions=chat_result.get("suggestions", [])
        )
        
        logger.info(f"AIå›å¤ç”ŸæˆæˆåŠŸ")
        return response
        
    except Exception as e:
        logger.error(f"AIèŠå¤©å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"AIèŠå¤©å¤±è´¥: {str(e)}")

@app.post("/generate/reply")
async def generate_reply_suggestion(email_data: EmailAnalysisRequest):
    """ç”Ÿæˆé‚®ä»¶å›å¤å»ºè®®"""
    try:
        if not openai.api_key:
            return {
                "suggested_reply": f"è°¢è°¢æ‚¨å…³äº'{email_data.subject}'çš„é‚®ä»¶ã€‚æˆ‘ä¼šä»”ç»†é˜…è¯»å¹¶å°½å¿«å›å¤æ‚¨ã€‚",
                "tone": "professional",
                "alternatives": [
                    "æ”¶åˆ°æ‚¨çš„é‚®ä»¶ï¼Œæˆ‘ä¼šè®¤çœŸå¤„ç†ã€‚",
                    "æ„Ÿè°¢æ‚¨çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬ä¼šåŠæ—¶è·Ÿè¿›ã€‚"
                ]
            }
        
        prompt = f"""
        è¯·ä¸ºä»¥ä¸‹é‚®ä»¶ç”Ÿæˆä¸€ä¸ªä¸“ä¸šã€å¾—ä½“çš„å›å¤ï¼š
        
        å‘ä»¶äººï¼š{email_data.sender}
        ä¸»é¢˜ï¼š{email_data.subject}
        å†…å®¹ï¼š{email_data.content}
        
        è¯·æä¾›ï¼š
        1. ä¸€ä¸ªä¸»è¦çš„å›å¤å»ºè®®ï¼ˆ100-200å­—ï¼‰
        2. è¯­è°ƒé£æ ¼ï¼ˆprofessional/friendly/formalï¼‰
        3. 2-3ä¸ªæ›¿ä»£å›å¤é€‰é¡¹
        
        ç”¨JSONæ ¼å¼è¿”å›ã€‚
        """
        
        response = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‚®ä»¶å›å¤åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç”Ÿæˆåˆé€‚çš„å›å¤å†…å®¹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=400,
                temperature=0.5
            )
        )
        
        result = response.choices[0].message.content
        try:
            return json.loads(result)
        except json.JSONDecodeError:
            return {
                "suggested_reply": "è°¢è°¢æ‚¨çš„é‚®ä»¶ï¼Œæˆ‘ä¼šåŠæ—¶å›å¤ã€‚",
                "tone": "professional",
                "alternatives": []
            }
            
    except Exception as e:
        logger.error(f"ç”Ÿæˆå›å¤å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå›å¤å¤±è´¥: {str(e)}")

@app.post("/classify/email")
async def classify_email(email_data: EmailAnalysisRequest):
    """é‚®ä»¶åˆ†ç±»åŠŸèƒ½"""
    try:
        # åŸºäºå…³é”®è¯çš„ç®€å•åˆ†ç±»
        subject_lower = email_data.subject.lower()
        content_lower = email_data.content.lower()
        
        categories = []
        if any(word in subject_lower or word in content_lower for word in ["å·¥ä½œ", "é¡¹ç›®", "ä¼šè®®", "meeting"]):
            categories.append("å·¥ä½œ")
        if any(word in subject_lower or word in content_lower for word in ["é€šçŸ¥", "å…¬å‘Š", "notice"]):
            categories.append("é€šçŸ¥")
        if any(word in subject_lower or word in content_lower for word in ["ç´§æ€¥", "åŠ æ€¥", "urgent"]):
            categories.append("ç´§æ€¥")
        if any(word in subject_lower or word in content_lower for word in ["è´¢åŠ¡", "æŠ¥é”€", "finance"]):
            categories.append("è´¢åŠ¡")
        
        if not categories:
            categories = ["ä¸€èˆ¬"]
        
        return {
            "categories": categories,
            "suggested_folder": categories[0] if categories else "å…¶ä»–",
            "confidence": 0.8 if len(categories) == 1 else 0.6
        }
        
    except Exception as e:
        logger.error(f"é‚®ä»¶åˆ†ç±»å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é‚®ä»¶åˆ†ç±»å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv('AI_SERVICE_PORT', 8001))
    logger.info(f"å¯åŠ¨AIæœåŠ¡ï¼Œç«¯å£: {port}")
    logger.info(f"OpenAI API: {'\u5df2\u914d\u7f6e' if openai.api_key else '\u672a\u914d\u7f6e\uff08\u4f7f\u7528\u6a21\u62df\u6a21\u5f0f\uff09'}")
    uvicorn.run(app, host="0.0.0.0", port=port)